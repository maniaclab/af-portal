{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Getting Started","text":"<p>!!! warning \"Documentation Moved\" This documentation has been migrated to the unified US ATLAS Analysis Facilities documentation.</p> <pre><code>Please update your bookmarks to the new location for the latest documentation covering all three US ATLAS Analysis Facilities (BNL, UChicago, and SLAC).\n</code></pre>"},{"location":"#logging-in-to-the-uchicago-analysis-facility","title":"Logging in to the UChicago Analysis Facility","text":"<p>First you will need to sign up on the Analysis Facility website.</p> <p>Please use your institutional or CERN identity (lxplus username) when signing up, as this will make the approval process smoother. You should set your institution to be your home institute. If e.g. you are from the University of Chicago but use your CERN credentials, you should set the institution under your profile to be the University of Chicago.</p> <p>As part of signing up you will need to upload an SSH Public Key.</p> <p>If you are not sure if you have generated an SSH Public Key before, try the following command (Mac/Linux) on your laptop to print the content of the file that contains the SSH Public Key:</p> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> <p>If the file exists, you should be able to copy the contents of this file to your profile on the AF website.</p> <pre><code>Important: Do not copy the contents of a file that does not end in .pub. You must only upload the public (.pub) part of the key.\n</code></pre> <p>If you do not have a public key (the file doesn't exist), you can generate one via the following command (Mac/Linux):</p> <pre><code>ssh-keygen -t rsa\n</code></pre> <p>Upload the resulting public key (ending in .pub) to your profile.</p> <p>Once you have uploaded a key, it will take a little bit of time to process your profile and add your account to the system. After 10-15 minutes, you ought to be able to login via SSH:</p> <pre><code>ssh &lt;username&gt;@login.af.uchicago.edu\n</code></pre> <p>If it does not work, please double check that you have been approved, have a public key uploaded and have waited at least 15 minutes. If you still have an issue, feel free to reach out to us for help.</p>"},{"location":"#using-analysis-facility-filesystems","title":"Using Analysis Facility Filesystems","text":"<p>The UChicago analysis facility has three filesystems that you should be aware of when running workloads. The table below describes their differences:</p> Filesystem Quota Path Backup Notes $HOME 100 GB /home/$USER Yes Solid-state filesystem, shared to all worker nodes $DATA 5 TB /data/$USER No CephFS filesystem, shared to all worker nodes $SCRATCH n/a /scratch No Ephemeral storage for workloads, local to worker nodes"},{"location":"Connect_VSCode_to_JupyterLab/","title":"Instructions to Connect VSCode to JupyterLab Kernel from UC AF","text":"<p>You can follow these steps to connect your Visual Studio Code to a JupyterLab kernel that is running on the University of Chicago Analysis Facility (UC AF). This allows you to work on your Jupyter notebooks using VS Code while utilizing the computational resources of UC AF.</p>"},{"location":"Connect_VSCode_to_JupyterLab/#steps","title":"Steps","text":"<ol> <li>Access Your JupyterLab:</li> <li>Visit UC AF JupyterLab to access your      JupyterLab environment.</li> <li> <p>You'll need to log in with your credentials and create your Jupyter server.</p> </li> <li> <p>Get the JupyterLab URL:</p> </li> <li> <p>Right click on the link to your JupyterLab instance, then copy it. This      link typically looks like      <code>https://ivukotic-notebook-1.notebook.af.uchicago.edu/?token=...</code>.</p> </li> <li> <p>Configure VS Code:</p> </li> <li>Open Visual Studio Code.</li> <li> <p>Install the Python and Jupyter extensions if you haven't already.</p> </li> <li> <p>Select Your Server and Kernel:</p> </li> <li>Open the notebook file you wish to work on in VS Code.</li> <li>Click on the kernel picker in the top right corner of the notebook editor</li> <li>Click the kernel dropdown \u2192 click \"Select Another Kernel...\" \u2192 then      \"Existing Jupyter Server...\".</li> <li>Paste your server URI there.</li> <li> <p>Select the kernel you want to use from your JupyterLab.</p> </li> <li> <p>Using the Remote Kernel:</p> </li> <li>Once the correct Kernel is selected, you can execute your notebook code      within VS Code, utilizing the UC AF's computational resources.</li> <li>If you encounter issues, ensure that VS Code is allowed through your      firewall if applicable, and your network allows communicating with the UC      AF nodes.</li> </ol> <p>Enjoy coding with the power of UC AF directly from VS Code!</p>"},{"location":"Connect_VSCode_to_JupyterLab/#notes","title":"Notes","text":"<ul> <li>Make sure your JupyterLab instance is running and remains active while you are   using VS Code to connect.</li> </ul>"},{"location":"VSCode_remote_ssh/","title":"Using VS Code Remote - SSH to Connect to UChicago AF (<code>login.af.uchicago.edu</code>)","text":"<p>This guide will walk you through installing the Remote - SSH plugin in Visual Studio Code (VS Code) and using it to connect to UChicago AF's login node (<code>login.af.uchicago.edu</code>).</p>"},{"location":"VSCode_remote_ssh/#step-1-install-remote-ssh-extension-in-vs-code","title":"\ud83e\udde9 Step 1: Install Remote - SSH Extension in VS Code","text":"<ol> <li>Open VS Code.</li> <li>Go to the Extensions view:</li> <li>Click on the square icon on the left sidebar (or press <code>Ctrl+Shift+X</code>).</li> <li>In the search bar, type: <code>Remote - SSH</code>.</li> <li>Locate the extension developed by Microsoft and click Install.</li> </ol>"},{"location":"VSCode_remote_ssh/#step-2-set-up-ssh-access-to-uchicago-af","title":"\ud83d\udd10 Step 2: Set Up SSH Access to UChicago AF","text":"<p>Before connecting, make sure:</p> <ul> <li>You have an account on UChicago AF.</li> <li>You can SSH into <code>login.af.uchicago.edu</code> from a terminal using:</li> </ul> <p><code>bash   ssh yourusername@login.af.uchicago.edu</code></p> <ul> <li>You have your SSH private key available locally.</li> </ul>"},{"location":"VSCode_remote_ssh/#step-3-configure-ssh-in-vs-code","title":"\u2699\ufe0f Step 3: Configure SSH in VS Code","text":"<ol> <li>Press <code>F1</code> or <code>Ctrl+Shift+P</code> to open the Command Palette.</li> <li>Type or select: <code>Remote-SSH: Add New SSH Host...</code></li> <li>Type: <code>ssh yourusername@login.af.uchicago.edu</code> (replace <code>yourusername</code> with    your actual AF username)</li> <li>You will be asked to <code>Select SSH configuration file to update</code>. We recommend    to use your <code>.ssh\\config</code></li> </ol>"},{"location":"VSCode_remote_ssh/#step-4-connect-to-uchicago-af-via-remote-ssh","title":"\ud83d\udd0c Step 4: Connect to UChicago AF via Remote - SSH","text":"<ol> <li>Press <code>F1</code> or <code>Ctrl+Shift+P</code>, then type:</li> </ol> <p><code>Remote-SSH: Connect to Host...</code></p> <ol> <li>Select <code>login.af.uchicago.edu</code>.</li> <li>VS Code will establish a connection and install the necessary server    components on the remote host.</li> <li>Once connected, you'll be in a remote VS Code environment, working    directly on <code>login.af.uchicago.edu</code>.</li> </ol>"},{"location":"VSCode_remote_ssh/#step-5-open-or-create-remote-projects","title":"\ud83d\udcc1 Step 5: Open or Create Remote Projects","text":"<ul> <li>Once connected, you can:</li> <li>Use File \u2192 Open Folder to open a directory on the remote machine.</li> <li>Open terminals, run scripts, and edit files as if you're working locally.</li> </ul>"},{"location":"VSCode_remote_ssh/#troubleshooting-tips","title":"\ud83d\udee0 Troubleshooting Tips","text":"<ul> <li>Connection fails: Try testing the SSH command manually in a terminal.</li> <li>Key permissions issue: Run <code>chmod 600 ~/.ssh/id_rsa</code></li> <li>Host key verification failed: Try deleting old entries from   <code>~/.ssh/known_hosts</code> if the remote host has changed.</li> </ul> <p>Enjoy a seamless coding experience on UChicago AF using the power of VS Code Remote Development! \ud83d\ude80</p>"},{"location":"coffeacasa/","title":"Coffea Casa","text":""},{"location":"coffeacasa/#prerequisites","title":"Prerequisites","text":"<p>The primary mode of analysis with coffea-casa is coffea. Coffea provides plenty of examples to users in its documentation A good starting point may be this tutorial on columnar analysis in coffea.</p> <p>Knowledge of Python is also assumed. Standard coffea analyses are contained within Jupyter Notebooks, which allow for dynamic, block-by-block execution of code. Coffea-casa employs the JupyterLab interface. JupyterLab is designed for hosting Jupyter Notebooks on the web, and permits the usage of additional features within its environment, including Git access, compatibility with cluster computing tools, and much, much more.</p> <p>If you aren't familiar with any of these tools, please click on the links above for additional resources, and get acquainted with how they work. If you want examples of how coffea-casa merges these tools, refer to the gallery of coffea-casa examples.</p>"},{"location":"coffeacasa/#access","title":"Access","text":"<p>Please use https://coffea.af.uchicago.edu as an access point to the Coffea-Casa Analysis Facility @ UChicago.</p> <p></p>"},{"location":"coffeacasa/#atlas-authz-authentication-instance","title":"ATLAS AuthZ Authentication Instance","text":"<p>Currently Coffea-Casa Analysis Facility @ UChicago can support any member of ATLAS.</p> <p>Sign in with your ATLAS CERN credential:</p> <p></p> <p></p>"},{"location":"coffeacasa/#docker-image-selection","title":"Docker Image Selection","text":"<p>For high efficient analysis using coffea package, powered with Dask and HTCondor please select:</p> <p></p> <p>After you will be forwarded to your personal Jupyterhub instance running at Analysis Facility @ UChicago:</p> <p></p>"},{"location":"coffeacasa/#cluster-resources-in-coffea-casa-analysis-facility-uchicago","title":"Cluster Resources in Coffea-Casa Analysis Facility @ UChicago","text":"<p>By default, the Coffea-casa Dask cluster should provide you with a scheduler and workers, which you can see by clicking on the colored Dask icon in the left sidebar.</p> <p></p> <p>As soon as you will start your computations, you will notice that available resources at the Coffea-Casa Analysis Facility @ UChicago can easily autoscale depending on available resources in the HTCondor pool at AF UChicago.</p> <p></p>"},{"location":"coffeacasa/#opening-a-new-console-or-file","title":"Opening a New Console or File","text":"<p>There are three ways by which you can open a new tab within coffea-casa. Two are located within the File menu at the very top of the JupyterLab interface: New and New Launcher.</p> <p></p> <p>The New dropdown menu allows you to open the console or a file of a specified format directly. The New Launcher option creates a new tab with buttons that permit you to launch a console or a new file, exactly like the interface you are shown when you first open coffea-casa.</p> <p>The final way is specific to the File Browser tab of the sidebar.</p> <p></p> <p>This behaves exactly like the New Launcher option above.</p> <pre><code>Regardless of the method you use to open a new file, the file will be saved to the current directory of your **File Browser.**\n</code></pre>"},{"location":"coffeacasa/#using-git","title":"Using Git","text":"<p>Cloning a repository in the Coffea-casa Analysis Facility @ UChicago is simple, though it can be a little confusing because it is spread across two tabs in the sidebar: the File Browser and the Git tabs.</p> <p>In order to clone a repository, first go to the Git tab. It should look like this:</p> <p></p> <p>Simply click the appropriate button (initialize a repository, or clone a repository) and you'll be hooked up to GitHub. This should then take you to the File Browser tab, which is where you can see all of the repositories you have cloned in your JupyterLab instance. The File Browser should look like this:</p> <p></p> <p>If you wish to change repositories, simply click the folder button to enter the root directory. If you are in the root directory, the Git tab will reset and allow you to clone another repository.</p> <p>If you wish to commit, push, or pull from the repository you currently have active in the File Browser, then you can return to the Git tab. It should change to look like this, so long as you have a repository open in the File Browser:</p> <p></p> <p>The buttons in the top right allow for pulling and pushing respectively. When you have edited files in a directory, they will show up under the Changed category, at which point you can hit the + to add them to a commit (at which point they will show up under Staged). Filling out the box at the bottom of the sidebar will file your commit, and prepare it for you to push.</p>"},{"location":"hardware/","title":"Hardware available at University of Chicago Analysis Facility (UC AF)","text":""},{"location":"hardware/#interactive-nodes","title":"Interactive Nodes","text":"Access Protocol Number of Nodes Monitoring ssh 2 Login nodes Jupyter Notebook 4 Notebooks"},{"location":"hardware/#batch-compute","title":"Batch Compute","text":"Queues Capacity (cores) Walltime Limit Notes long 1520 72 Hours The long queue jobs run on the hyperconverged nodes short 1280 4 Hours The short queue jobs can run on both the fast compute nodes and the hyperconverged nodes"},{"location":"hardware/#storage-spaces","title":"Storage Spaces","text":"Storage Type Capacity Default Quota Notes /data Ceph Filesystem 1PB 5TB Storage provided by the batch worker nodes. Two types of disks: spinning disks from hyperconverged nodes form the regular pool (in production), NVMe disks from the fast compute nodes form the fast pool (to be configured) /home NFS filesystem 33TB 100GB /scratch SSD Local Filesystem 2TB on the hyperconverged nodes, 6TB on the fast compute nodes N/A"},{"location":"hardware/#hardware-specifications","title":"Hardware Specifications","text":"Node Type Number of Nodes Processor Per Node Cores Per Node Memory Per Node Storage Per Node Notes Hyperconverged 19 Two AMD EPYC 7402 CPUs at 2.8 GHz 48C/96T 512 GB DDR4 SDRAM Two 960 GB SSD, two 2TB NVMe and twelve 16 TB spinning disks. 3 nodes provided by, and for, UChicago ATLAS group Fast Compute 16 Two Intel(R) Xeon(R) Gold 6348 CPU at 2.60 GHz 56C/112T 384 GB DDR4 SDRAM Ten 3.2 TB NVMe Provided by, and for, IRIS-HEP SSL Interactive Nodes 8 Two AMD EPYC 7402 CPUs at 2.8 GHz 48C/96T 256 GB DDR4 SDRAM Two 960 GB SSD 2 nodes provided by, and for, UChicago ATLAS group Xcache Nodes 1 Two Intel(R) Xeon(R) Silver 4214 CPU @ 2.20GHz 24C/48T 192 GB DDR4 SDRAM Twenty four 1.5 TB NVMe Two 25 Gbps network links Node Type Number of Nodes Processor Per Node Cores Per Node Memory Per Node GPUs Per Node (Mem) Storage Per Node GPU A 2 Two AMD EPYC 7543 32-Core Processor 64C/128T 512 GB DDR4 SDRAM Four NVIDIA A100 (40G) One 1.5TB NVMe GPU B 1 Two Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz 24C/48T 96 GB DDR4 SDRAM Four Tesla V100 (16G) Three 220GB SSD GPU C 3 Two Intel(R) Xeon(R) Gold 6146 CPU @ 3.20GHz 24C/48T 192 GB DDR4 SDRAM Eight NVIDIA GeForce RTX 2080 Ti (12G) Six 450GB SSD GPU D 1 Two Intel(R) Xeon(R) CPU E5-2687W v4 @ 3.00GHz 24C 128 GB DDR4 SDRAM Eight NVIDIA GeForce GTX 1080 Ti (12G) Six 450GB SSD"},{"location":"job_submission/","title":"Submitting to the Analysis Facility","text":"<p>The UChicago Analysis Facility uses HTCondor for batch workloads. You will need to define an executable script and a \"submit\" file that describes your job. A simple job that loads the ATLAS environment looks something like this:</p> <p>Job script, called myjob.sh:</p> <pre><code>#!/bin/bash\nexport ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase\nexport ALRB_localConfigDir=$HOME/localConfig\nsource ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh\n# at this point, you can lsetup root, rucio, athena, etc..\n</code></pre> <p>Submit file, called myjob.sub:</p> <pre><code>Universe = vanilla\n\nOutput = myjob.$(Cluster).$(Process).out\nError = myjob.$(Cluster).$(Process).err\nLog = myjob.log\n\nExecutable = myjob.sh\n\nrequest_memory = 1GB\nrequest_cpus = 1\n\nQueue 1\n</code></pre> <p>By default, your jobs will be submitted via the long queue, with no time limits. However, in order to make your jobs start faster, you can request specifically to send them via the short queue. This is for jobs that should take under 4 hours. For this purpose, you need to add a line in your submit file:</p> <pre><code>+queue=\"short\"\n</code></pre> <p>The condor_submit command is used to queue jobs:</p> <pre><code>$ condor_submit myjob.sub\nSubmitting job(s).\n1 job(s) submitted to cluster 17.\n</code></pre> <p>And the condor_q command is used to view the queue:</p> <pre><code>[lincolnb@login01 ~]$ condor_q\n-- Schedd: head01.af.uchicago.edu : &amp;lt;192.170.240.14:9618?... @ 07/22/21 11:28:26\nOWNER    BATCH_NAME    SUBMITTED   DONE   RUN    IDLE  TOTAL JOB_IDS\nlincolnb ID: 17       7/22 11:27      _      1      _      1 17.0\n\nTotal for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended\nTotal for lincolnb: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended\nTotal for all users: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended\n</code></pre>"},{"location":"job_submission/#configuring-your-jobs-to-use-an-x509-proxy-certificate","title":"Configuring your jobs to use an X509 Proxy Certificate","text":"<p>If you need to use an X509 Proxy, e.g. to access ATLAS Data, you will want to copy your X509 certificate to the Analysis Facility.</p> <p>Store your certificate in <code>$HOME/.globus</code> and create a ATLAS VOMS proxy in the usual way:</p> <pre><code>export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase\nexport ALRB_localConfigDir=$HOME/localConfig\nsource ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh\nlsetup emi\nvoms-proxy-init -voms atlas -out $HOME/x509proxy\n</code></pre> <p>You will want to generate the proxy on, or copy it to, the shared $HOME filesystem so that the HTCondor scheduler can find and read the proxy. With the following additions to your jobscript, HTCondor will configure the job environment automatically for X509 authenticated data access:</p> <pre><code>use_x509userproxy = true\nx509userproxy = /home/YOURUSERNAME/x509proxy\n</code></pre> <p>E.g., in the job above for the user lincolnb:</p> <pre><code>Universe = vanilla\n\nOutput = myjob.$(Cluster).$(Process).out\nError = myjob.$(Cluster).$(Process).err\nLog = myjob.log\n\nExecutable = myjob.sh\n\nuse_x509userproxy = true\nx509userproxy = /home/lincolnb/x509proxy\n\nrequest_memory = 1GB\nrequest_cpus = 1\n\nQueue 1\n</code></pre>"},{"location":"job_submission/#limits","title":"Limits","text":"<p>Currently there is a limit to max of 64GB RAM and max 16 cores/CPUs that you can request per job.</p>"},{"location":"job_submission/#using-analysis-facility-filesystems","title":"Using Analysis Facility Filesystems","text":"<p>When submitting jobs, you should try to use the local scratch filesystem whenever possible. This will help you be a \"good neighbor\" to other users on the system, and reduce overall stress on the shared filesystems, which can lead to slowness, downtimes, etc. By default, jobs start in the $SCRATCH directory on the worker nodes. Output data will need to be staged to the shared filesystem or it will be lost!</p> <p>In the following example, data is read from Rucio, we pretend to process it, and then push a small output copied back to the $HOME filesystem. It assumes your X509 proxy certificate is valid and in your home directory.</p> <pre><code>#!/bin/bash\nexport ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase\nexport ALRB_localConfigDir=$HOME/localConfig\nsource ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh\nlsetup rucio\nrucio --verbose download --rse MWT2_DATADISK data16_13TeV:AOD.11071822._001488.pool.root.1\n\n# You can run things like asetup as well\nasetup AnalysisBase,21.2.81\n\n# This is where you would do your data analysis via AnalysisBase, etc. We will\n# just pretend to do that, and truncate the file to simulate generating an\n# output file. This is definitely not what you want to do in a real analysis!\ncd data16_13TeV\ntruncate --size 10MB AOD.11071822._001488.pool.root.1\ncp AOD.11071822._001488.pool.root.1 $HOME/myjob.output\n</code></pre> <p>It gets submitted in the usual way:</p> <pre><code>Universe = vanilla\n\nOutput = myjob.$(Cluster).$(Process).out\nError = myjob.$(Cluster).$(Process).err\nLog = myjob.log\n\nExecutable = myjob.sh\n\nuse_x509userproxy = true\nx509userproxy = /home/lincolnb/x509proxy\n\nrequest_memory = 1GB\nrequest_cpus = 1\n\nQueue 1\n</code></pre> <p>And then:</p> <pre><code>$ condor_submit myjob.sub\nSubmitting job(s).\n1 job(s) submitted to cluster 17.\n</code></pre>"},{"location":"job_submission/#using-docker-singularity-containers-advanced","title":"Using Docker / Singularity containers (Advanced)","text":"<p>Some users may want to bring their own container-based workloads to the Analysis Facility. We support both Docker-based jobs as well as Singularity-based jobs. Additionally, the CVMFS repository unpacked.cern.ch is mounted on all nodes.</p> <p>If, for whatever reason, you wanted to run a Debian Linux-based container on the Analysis Facility, it would be as simple as the following Job file:</p> <pre><code>universe                = docker\ndocker_image            = debian\nexecutable              = /bin/cat\narguments               = /etc/hosts\nshould_transfer_files   = YES\nwhen_to_transfer_output = ON_EXIT\noutput                  = out.$(Process)\nerror                   = err.$(Process)\nlog                     = log.$(Process)\nrequest_memory          = 1000M\nqueue 1\n</code></pre> <p>Similarly, if you would like to run a Singularity container, such as the ones provided in th unpacked.cern.ch CVMFS repo, you can submit a normal vanilla universe job, with a job executable that looks something like the following:</p> <pre><code>#!/bin/bash\nsingularity run -B /cvmfs -B /home /cvmfs/unpacked.cern.ch/registry.hub.docker.com/atlas/rucio-clients:default rucio --version\n</code></pre> <p>Replacing the <code>rucio-clients:default</code> container and <code>rucio --version</code> executable with your preferred software.</p>"},{"location":"local-ddm/","title":"Local DDM","text":""},{"location":"local-ddm/#accessing-data-directly-from-ddms","title":"Accessing data directly from DDMs","text":"<p>If your dataset is available at the local DDM endpoint (eg. MWT2_UC_LOCALGROUPDISK), it would be very performat to read the data directly from it. First you need to discover paths to the files of your dataset. This is easiest done in Rucio. You would issue a command like:</p> <pre><code>setupATLAS\nlsetup rucio\nrucio\n# get your grid proxy\nvoms-proxy-init -voms atlas\nrucio list-file-replicas user.ivukotic:xcache.test.dat --protocols root --pfns\n</code></pre> <p>This will give you a list of paths to all of the files and all the replicas in your dataset. You can also limit it to only paths to specific RSES:</p> <pre><code>rucio list-file-replicas user.ivukotic:xcache.test.dat --protocols root --pfns --rses MWT2_UC_LOCALGROUPDISK\n# output will look like this:\n# root://fax.mwt2.org:1094//pnfs/uchicago.edu/atlaslocalgroupdisk/rucio/user/ivukotic/7d/9b/xcache.test.dat\n</code></pre> <p>If you need only some files, you can simply grep or awk for them and save filepaths to txt file that your jobs will use.</p>"},{"location":"ml_platform/","title":"JupyterLab","text":"<p>To support machine learning code development, our users can deploy one or more private JupyterLab applications.</p> <p>To encourage fair sharing these applications are time limited. We also ask users to request only the resources that they need.</p>"},{"location":"ml_platform/#limitations","title":"Limitations","text":"<ul> <li>You can request 1 to 16 CPU cores.</li> <li>You can request 1 to 32 GB of memory.</li> <li>You can request 0 to 7 GPU instances.</li> <li>A notebook can have lifetime of up to 72 hours.</li> <li>You can select a GPU model based on its memory size. If you request a GPU,   please make sure the GPU is available, by clicking on the icon next to GPU   memory.</li> </ul>"},{"location":"ml_platform/#selecting-gpu-memory-and-instances","title":"Selecting GPU memory and instances","text":"<p>The AF cluster has four NVIDIA A100 GPUs. Each GPU can be partitioned into seven GPU instances. This means the AF cluster can have up to 28 GPU instances running in parallel.</p> <p>A user can request 0 to 7 GPU instances as a resource for the notebook. A user can request 40,836 MB of memory for an entire A100 GPU, or 4864 MB of memory for a MIG instance.</p>"},{"location":"ml_platform/#selecting-a-docker-image","title":"Selecting a Docker image","text":"<p>Users can choose from five images:</p> <ul> <li><code>ml_platform:latest</code> - Based on NVIDIA image, it has most of the ML packages   (Tensorflow, Keras, ScikitLearn,...) preinstalled, and a small tutorial with   example codes in /ML_platform_tests/tutorial, it supports NVidia GPUs and has   ROOT preinstalled.</li> <li><code>ml_platform:conda</code> - comes with full anaconda.</li> <li><code>ml_platform:julia</code> - with Julia programming language</li> <li><code>ml_platform:lava</code> - with Intel Lava neuromorphic computing framework</li> <li><code>ml_platform:centos</code></li> <li><code>AB-stable</code> - based on AnalysisBase</li> <li><code>AB-dev</code> - based on AnalysisBase but with cutting edge uproot, dask, awkward   arrays, etc.</li> </ul> <p>For software additions and upgrades please contact ivukotic@uchicago.edu.</p>"},{"location":"ml_platform/#tutorials","title":"Tutorials","text":"<p>Basic usage of the platform can be experienced by running the set of tutorials that come preinstalled with both latest and conda image.</p>"},{"location":"ml_platform/#running-in-conda","title":"Running in conda","text":"<p>To run tutorial in conda environment, one first has to initialize conda. Simply open a jupyter lab terminal, and execute: conda init. Close that terminal and open a new one. This will drop you in (base) conda environment. You may now switch to a HEP relevant environment by executing: conda activate codas-hep.</p>"},{"location":"servicex/","title":"ServiceX","text":"<p>When dealing with very large datasets it is often better to do initial data filtering, augmentation using <code>ServiceX &lt;https://iris-hep.org/projects/servicex&gt;</code>_. ServiceX transformation produces output as an Awkward Array. The array can then be used in a regular Coffea processing. Here a scheme explaining the workflow:</p> <p></p> <p>There are two different, UC AF deployed ServiceX instances. The only difference between them is the type of input data they are capable of processing. Uproot processes any kind of \"flat\" ROOT files, while xAOD processes only Rucio registered xAOD files.</p> <p>To use them one has to register and get approved. Sign in will lead you to a Globus registration page, where you may choose to use account connected to your institution:</p> <p></p> <p>Once approved, you will be able to see status of your requests in the dashboard:</p> <p></p> <p>For your code to be able to authenticate your requests, you need to download a servicex.yaml file, that should be placed in your working directory. The file is downloaded from your profile page:</p> <p></p> <p>For an example analysis using ServiceX and Coffea look here.</p>"},{"location":"servicex/#more-examples","title":"More Examples","text":"<p>Columnar data analysis with DAOD_PHYSLITE here.</p> <p>ServiceX analysis on ROOT files, with Coffea, and TRExFitter here.</p>"},{"location":"xcache/","title":"XCache","text":"<p>To speed up remote data access, Analysis Facility maintains an XCache server (managed through SLATE), with 25 x 1.5 TB NVMes and 2x25 Gbps NIC.</p> <p>Users that want do access remote data of their own (on EOS or elsewhere) can manually add the prefix <code>root://xcache.af.uchicago.edu:1094//</code> to their root paths, eg:</p> <p>If the original path is eg.:</p> <pre><code>root://someserver.org:1094//atlaslocalgroupdisk/rucio/user/mgeyik/63/c4/user.mgeyik.26617246._000006.out.root\n</code></pre> <p>make it:</p> <pre><code>root://xcache.af.uchicago.edu:1094//root://someserver.org:1094//atlaslocalgroupdisk/rucio/user/mgeyik/63/c4/user.mgeyik.26617246._000006.out.root\n</code></pre> <p>While initial data access will be slightly slower, following accesses will be lightning fast.</p> <p>NB. ServiceX uses the XCache by default.</p>"}]}